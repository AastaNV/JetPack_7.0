From 25c9fd396ca2fe1a07fe4fb18bb52c0e2c23deaf Mon Sep 17 00:00:00 2001
From: Your Name <you@example.com>
Date: Thu, 4 Sep 2025 20:28:12 +0000
Subject: [PATCH] stress test

---
 .../matrixMulCUBLAS/matrixMulCUBLAS.cpp       | 575 ++++++++++--------
 1 file changed, 311 insertions(+), 264 deletions(-)

diff --git a/Samples/4_CUDA_Libraries/matrixMulCUBLAS/matrixMulCUBLAS.cpp b/Samples/4_CUDA_Libraries/matrixMulCUBLAS/matrixMulCUBLAS.cpp
index 0a5adb0b..d65b3f1c 100644
--- a/Samples/4_CUDA_Libraries/matrixMulCUBLAS/matrixMulCUBLAS.cpp
+++ b/Samples/4_CUDA_Libraries/matrixMulCUBLAS/matrixMulCUBLAS.cpp
@@ -65,15 +65,15 @@
 
 // Utilities and system includes
 #include <assert.h>
-#include <helper_string.h> // helper for shared functions common to CUDA Samples
+#include <helper_string.h>  // helper for shared functions common to CUDA Samples
 
 // CUDA runtime
-#include <cublas_v2.h>
 #include <cuda_runtime.h>
+#include <cublas_v2.h>
 
 // CUDA and CUBLAS functions
-#include <helper_cuda.h>
 #include <helper_functions.h>
+#include <helper_cuda.h>
 
 #ifndef min
 #define min(a, b) ((a < b) ? a : b)
@@ -83,11 +83,56 @@
 #endif
 
 // Optional Command-line multiplier for matrix sizes
-typedef struct _matrixSize
-{
-    unsigned int uiWA, uiHA, uiWB, uiHB, uiWC, uiHC;
+typedef struct _matrixSize {
+  unsigned int uiWA, uiHA, uiWB, uiHB, uiWC, uiHC;
 } sMatrixSize;
 
+// Modified from helper_image.h to support half precision arguments
+inline bool sdkCompareL2fe(const __half *reference, const __half *data,
+                           const unsigned int len, const float epsilon) {
+  assert(epsilon >= 0);
+
+  float error = 0;
+  float ref = 0;
+
+  for (unsigned int i = 0; i < len; ++i) {
+    float diff = (float)reference[i] - (float)data[i];
+    error += diff * diff;
+    ref += (float)reference[i] * (float)reference[i];
+  }
+
+  float normRef = sqrtf(ref);
+
+  if (fabs(ref) < 1e-7) {
+#ifdef _DEBUG
+    std::cerr << "ERROR, reference l2-norm is 0\n";
+#endif
+    return false;
+  }
+
+  float normError = sqrtf(error);
+  error = normError / normRef;
+  bool result = error < epsilon;
+#ifdef _DEBUG
+
+  if (!result) {
+    std::cerr << "ERROR, l2-norm error " << error << " is greater than epsilon "
+              << epsilon << "\n";
+  }
+
+#endif
+
+  return result;
+}
+
+inline int idx_n(int row, int col, int height, int width) {
+    return row * width + col;
+}
+
+inline int idx_t(int row, int col, int height, int width) {
+    return col * height + row;
+}
+
 ////////////////////////////////////////////////////////////////////////////////
 //! Compute reference data set matrix multiply on CPU
 //! C = A * B
@@ -97,292 +142,294 @@ typedef struct _matrixSize
 //! @param hA         height of matrix A
 //! @param wB         width of matrix B
 ////////////////////////////////////////////////////////////////////////////////
-void matrixMulCPU(float *C, const float *A, const float *B, unsigned int hA, unsigned int wA, unsigned int wB)
-{
-    for (unsigned int i = 0; i < hA; ++i)
-        for (unsigned int j = 0; j < wB; ++j) {
-            double sum = 0;
-
-            for (unsigned int k = 0; k < wA; ++k) {
-                double a = A[i * wA + k];
-                double b = B[k * wB + j];
-                sum += a * b;
-            }
-
-            C[i * wB + j] = (float)sum;
-        }
+void matrixMulCPU(__half *C, const __half *A, const __half *B, unsigned int hA, unsigned int wA, unsigned int wB, const bool ta, const bool tb) {
+  auto idx_a = (ta) ? idx_t : idx_n;
+  auto idx_b = (tb) ? idx_t : idx_n;
+  for (unsigned int i = 0; i < hA; ++i)
+    for (unsigned int j = 0; j < wB; ++j) {
+      float sum = 0;
+
+      for (unsigned int k = 0; k < wA; ++k) {
+        __half a = A[i * wA + k];
+        __half b = B[k * wB + j];
+        sum += float(a * b);
+      }
+
+      C[i * wB + j] = sum;
+    }
 }
 
-// Allocates a matrix with random float entries.
-void randomInit(float *data, int size)
-{
-    for (int i = 0; i < size; ++i)
-        data[i] = rand() / (float)RAND_MAX;
+// Allocates a matrix with random __half entries.
+void randomInit(__half *data, int size) {
+  for (int i = 0; i < size; ++i) data[i] = (__half)(rand() / (float)(RAND_MAX));
 }
 
-void printDiff(float *data1, float *data2, int width, int height, int iListLength, float fListTol)
-{
-    printf("Listing first %d Differences > %.6f...\n", iListLength, fListTol);
-    int i, j, k;
-    int error_count = 0;
-
-    for (j = 0; j < height; j++) {
-        if (error_count < iListLength) {
-            printf("\n  Row %d:\n", j);
-        }
-
-        for (i = 0; i < width; i++) {
-            k           = j * width + i;
-            float fDiff = fabs(data1[k] - data2[k]);
-
-            if (fDiff > fListTol) {
-                if (error_count < iListLength) {
-                    printf("    Loc(%d,%d)\tCPU=%.5f\tGPU=%.5f\tDiff=%.6f\n", i, j, data1[k], data2[k], fDiff);
-                }
+void printDiff(__half *data1, __half *data2, int width, int height,
+		int iListLength, float fListTol) {
+  printf("Listing first %d Differences > %.6f...\n", iListLength, fListTol);
+  int i, j, k;
+  int error_count = 0;
 
-                error_count++;
-            }
-        }
+  for (j = 0; j < height; j++) {
+    if (error_count < iListLength) {
+      printf("\n  Row %d:\n", j);
     }
 
-    printf(" \n  Total Errors = %d\n", error_count);
-}
-
-void initializeCUDA(int argc, char **argv, int &devID, int &iSizeMultiple, sMatrixSize &matrix_size)
-{
-    // By default, we use device 0, otherwise we override the device ID based on
-    // what is provided at the command line
-    cudaError_t error;
-    devID = 0;
+    for (i = 0; i < width; i++) {
+      k = j * width + i;
+      float fDiff = fabs((float)data1[k] - (float)data2[k]);
 
-    devID = findCudaDevice(argc, (const char **)argv);
+      if (fDiff > fListTol) {
+        if (error_count < iListLength) {
+          printf("    Loc(%d,%d)\tCPU=%.5f\tGPU=%.5f\tDiff=%.6f\n", i, j,
+                 (float)data1[k], (float)data2[k], fDiff);
+        }
 
-    if (checkCmdLineFlag(argc, (const char **)argv, "sizemult")) {
-        iSizeMultiple = getCmdLineArgumentInt(argc, (const char **)argv, "sizemult");
+        error_count++;
+      }
     }
+  }
 
-    iSizeMultiple = min(iSizeMultiple, 10);
-    iSizeMultiple = max(iSizeMultiple, 1);
-
-    cudaDeviceProp deviceProp;
-
-    error = cudaGetDeviceProperties(&deviceProp, devID);
-
-    if (error != cudaSuccess) {
-        printf("cudaGetDeviceProperties returned error code %d, line(%d)\n", error, __LINE__);
-        exit(EXIT_FAILURE);
-    }
+  printf(" \n  Total Errors = %d\n", error_count);
+}
 
-    printf("GPU Device %d: \"%s\" with compute capability %d.%d\n\n",
-           devID,
-           deviceProp.name,
-           deviceProp.major,
-           deviceProp.minor);
-
-    int block_size = 32;
-
-    matrix_size.uiWA = 3 * block_size * iSizeMultiple;
-    matrix_size.uiHA = 4 * block_size * iSizeMultiple;
-    matrix_size.uiWB = 2 * block_size * iSizeMultiple;
-    matrix_size.uiHB = 3 * block_size * iSizeMultiple;
-    matrix_size.uiWC = 2 * block_size * iSizeMultiple;
-    matrix_size.uiHC = 4 * block_size * iSizeMultiple;
-
-    printf("MatrixA(%u,%u), MatrixB(%u,%u), MatrixC(%u,%u)\n",
-           matrix_size.uiHA,
-           matrix_size.uiWA,
-           matrix_size.uiHB,
-           matrix_size.uiWB,
-           matrix_size.uiHC,
-           matrix_size.uiWC);
-
-    if (matrix_size.uiWA != matrix_size.uiHB || matrix_size.uiHA != matrix_size.uiHC
-        || matrix_size.uiWB != matrix_size.uiWC) {
-        printf("ERROR: Matrix sizes do not match!\n");
-        exit(-1);
-    }
+void initializeCUDA(int argc, char **argv, int &devID, int &m, int &n, int &k,
+                    sMatrixSize &matrix_size) {
+  // By default, we use device 0, otherwise we override the device ID based on
+  // what is provided at the command line
+  cudaError_t error;
+  devID = 0;
+
+  devID = findCudaDevice(argc, (const char **)argv);
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "m")) {
+    m =
+        getCmdLineArgumentInt(argc, (const char **)argv, "m");
+  }
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "n")) {
+    n =
+        getCmdLineArgumentInt(argc, (const char **)argv, "n");
+  }
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "k")) {
+    k =
+        getCmdLineArgumentInt(argc, (const char **)argv, "k");
+  }
+
+  if ( m%32!=0 || m>8192 || m<32 ) {
+    printf("Invalid value: m,n,k need to be the multiple of 32 and between [32, 8192]\n");
+    assert(0);
+  }
+  if ( n%32!=0 || n>8192 || n<32 ) {
+    printf("Invalid value: m,n,k need to be the multiple of 32 and between [32, 8192]\n");
+    assert(0);
+  }
+  if ( k%32!=0 || k>8192 || k<32 ) {
+    printf("Invalid value: m,n,k need to be the multiple of 32 and between [32, 8192]\n");
+    assert(0);
+  }
+
+  cudaDeviceProp deviceProp;
+
+  error = cudaGetDeviceProperties(&deviceProp, devID);
+
+  if (error != cudaSuccess) {
+    printf("cudaGetDeviceProperties returned error code %d, line(%d)\n", error,
+           __LINE__);
+    exit(EXIT_FAILURE);
+  }
+
+  printf("GPU Device %d: \"%s\" with compute capability %d.%d\n\n", devID,
+         deviceProp.name, deviceProp.major, deviceProp.minor);
+
+  matrix_size.uiWA = n;
+  matrix_size.uiHA = m;
+  matrix_size.uiWB = k;
+  matrix_size.uiHB = n;
+  matrix_size.uiWC = k;
+  matrix_size.uiHC = m;
+
+  printf("MatrixA(%u,%u), MatrixB(%u,%u), MatrixC(%u,%u)\n", matrix_size.uiHA,
+         matrix_size.uiWA, matrix_size.uiHB, matrix_size.uiWB, matrix_size.uiHC,
+         matrix_size.uiWC);
+
+  if (matrix_size.uiWA != matrix_size.uiHB ||
+      matrix_size.uiHA != matrix_size.uiHC ||
+      matrix_size.uiWB != matrix_size.uiWC) {
+    printf("ERROR: Matrix sizes do not match!\n");
+    exit(-1);
+  }
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 //! Run a simple test matrix multiply using CUBLAS
 ////////////////////////////////////////////////////////////////////////////////
-int matrixMultiply(int argc, char **argv, int devID, sMatrixSize &matrix_size)
-{
-    cudaDeviceProp deviceProp;
-
-    checkCudaErrors(cudaGetDeviceProperties(&deviceProp, devID));
-
-    int block_size = 32;
-
-    // set seed for rand()
-    srand(2006);
-
-    // allocate host memory for matrices A and B
-    unsigned int size_A     = matrix_size.uiWA * matrix_size.uiHA;
-    unsigned int mem_size_A = sizeof(float) * size_A;
-    float       *h_A        = (float *)malloc(mem_size_A);
-    unsigned int size_B     = matrix_size.uiWB * matrix_size.uiHB;
-    unsigned int mem_size_B = sizeof(float) * size_B;
-    float       *h_B        = (float *)malloc(mem_size_B);
-
-    // set seed for rand()
-    srand(2006);
-
-    // initialize host memory
-    randomInit(h_A, size_A);
-    randomInit(h_B, size_B);
-
-    // allocate device memory
-    float       *d_A, *d_B, *d_C;
-    unsigned int size_C     = matrix_size.uiWC * matrix_size.uiHC;
-    unsigned int mem_size_C = sizeof(float) * size_C;
-
-    // allocate host memory for the result
-    float *h_C      = (float *)malloc(mem_size_C);
-    float *h_CUBLAS = (float *)malloc(mem_size_C);
-
-    checkCudaErrors(cudaMalloc((void **)&d_A, mem_size_A));
-    checkCudaErrors(cudaMalloc((void **)&d_B, mem_size_B));
-    checkCudaErrors(cudaMemcpy(d_A, h_A, mem_size_A, cudaMemcpyHostToDevice));
-    checkCudaErrors(cudaMemcpy(d_B, h_B, mem_size_B, cudaMemcpyHostToDevice));
-    checkCudaErrors(cudaMalloc((void **)&d_C, mem_size_C));
-
-    // setup execution parameters
-    dim3 threads(block_size, block_size);
-    dim3 grid(matrix_size.uiWC / threads.x, matrix_size.uiHC / threads.y);
-
-    // create and start timer
-    printf("Computing result using CUBLAS...");
-
-    // execute the kernel
-    int nIter = 30;
-
-    // CUBLAS version 2.0
-    {
-        const float    alpha = 1.0f;
-        const float    beta  = 0.0f;
-        cublasHandle_t handle;
-        cudaEvent_t    start, stop;
-
-        checkCudaErrors(cublasCreate(&handle));
-
-        // Perform warmup operation with cublas
-        checkCudaErrors(cublasSgemm(handle,
-                                    CUBLAS_OP_N,
-                                    CUBLAS_OP_N,
-                                    matrix_size.uiWB,
-                                    matrix_size.uiHA,
-                                    matrix_size.uiWA,
-                                    &alpha,
-                                    d_B,
-                                    matrix_size.uiWB,
-                                    d_A,
-                                    matrix_size.uiWA,
-                                    &beta,
-                                    d_C,
-                                    matrix_size.uiWB));
-
-        // Allocate CUDA events that we'll use for timing
-        checkCudaErrors(cudaEventCreate(&start));
-        checkCudaErrors(cudaEventCreate(&stop));
-
-        // Record the start event
-        checkCudaErrors(cudaEventRecord(start, NULL));
-
-        for (int j = 0; j < nIter; j++) {
-            // note cublas is column primary!
-            // need to transpose the order
-            checkCudaErrors(cublasSgemm(handle,
-                                        CUBLAS_OP_N,
-                                        CUBLAS_OP_N,
-                                        matrix_size.uiWB,
-                                        matrix_size.uiHA,
-                                        matrix_size.uiWA,
-                                        &alpha,
-                                        d_B,
-                                        matrix_size.uiWB,
-                                        d_A,
-                                        matrix_size.uiWA,
-                                        &beta,
-                                        d_C,
-                                        matrix_size.uiWB));
-        }
-
-        printf("done.\n");
-
-        // Record the stop event
-        checkCudaErrors(cudaEventRecord(stop, NULL));
-
-        // Wait for the stop event to complete
-        checkCudaErrors(cudaEventSynchronize(stop));
-
-        float msecTotal = 0.0f;
-        checkCudaErrors(cudaEventElapsedTime(&msecTotal, start, stop));
-
-        // Compute and print the performance
-        float  msecPerMatrixMul  = msecTotal / nIter;
-        double flopsPerMatrixMul = 2.0 * (double)matrix_size.uiHC * (double)matrix_size.uiWC * (double)matrix_size.uiHB;
-        double gigaFlops         = (flopsPerMatrixMul * 1.0e-9f) / (msecPerMatrixMul / 1000.0f);
-        printf("Performance= %.2f GFlop/s, Time= %.3f msec, Size= %.0f Ops\n",
-               gigaFlops,
-               msecPerMatrixMul,
-               flopsPerMatrixMul);
-
-        // copy result from device to host
-        checkCudaErrors(cudaMemcpy(h_CUBLAS, d_C, mem_size_C, cudaMemcpyDeviceToHost));
-
-        // Destroy the handle
-        checkCudaErrors(cublasDestroy(handle));
+int matrixMultiply(int argc, char **argv, int devID, sMatrixSize &matrix_size, bool ta, bool tb) {
+  cudaDeviceProp deviceProp;
+
+  checkCudaErrors(cudaGetDeviceProperties(&deviceProp, devID));
+
+  int block_size = 32;
+
+  // set seed for rand()
+  srand(2006);
+
+  // allocate host memory for matrices A and B
+  unsigned int size_A = matrix_size.uiWA * matrix_size.uiHA;
+  unsigned int mem_size_A = sizeof(__half) * size_A;
+  __half *h_A = (__half *)malloc(mem_size_A);
+  unsigned int size_B = matrix_size.uiWB * matrix_size.uiHB;
+  unsigned int mem_size_B = sizeof(__half) * size_B;
+  __half *h_B = (__half *)malloc(mem_size_B);
+
+  // set seed for rand()
+  srand(2006);
+
+  // initialize host memory
+  randomInit(h_A, size_A);
+  randomInit(h_B, size_B);
+
+  // allocate device memory
+  __half *d_A, *d_B, *d_C;
+  unsigned int size_C = matrix_size.uiWC * matrix_size.uiHC;
+  unsigned int mem_size_C = sizeof(__half) * size_C;
+
+  // allocate host memory for the result
+  __half *h_C      = (__half *) malloc(mem_size_C);
+  __half *h_CUBLAS = (__half *) malloc(mem_size_C);
+
+  checkCudaErrors(cudaMalloc((void **)&d_A, mem_size_A));
+  checkCudaErrors(cudaMalloc((void **)&d_B, mem_size_B));
+  checkCudaErrors(cudaMemcpy(d_A, h_A, mem_size_A, cudaMemcpyHostToDevice));
+  checkCudaErrors(cudaMemcpy(d_B, h_B, mem_size_B, cudaMemcpyHostToDevice));
+  checkCudaErrors(cudaMalloc((void **)&d_C, mem_size_C));
+
+  // setup execution parameters
+  dim3 threads(block_size, block_size);
+  dim3 grid(matrix_size.uiWC / threads.x, matrix_size.uiHC / threads.y);
+
+  // create and start timer
+  printf("Computing result using CUBLAS...");
+
+  // execute the kernel
+  int nIter = INT_MAX;
+
+  // CUBLAS version 2.0
+  {
+    cublasOperation_t trans_A = (ta) ? CUBLAS_OP_T : CUBLAS_OP_N;
+    cublasOperation_t trans_B = (tb) ? CUBLAS_OP_T : CUBLAS_OP_N;
+    int m = matrix_size.uiWC;
+    int n = matrix_size.uiHC;
+    int k = matrix_size.uiWA;
+    int lda = (trans_A == CUBLAS_OP_N) ? k : n;
+    int ldb = (trans_B == CUBLAS_OP_N) ? m : k;
+    int ldc = m;
+    const __half alpha = 1.0f;
+    const __half beta  = 0.0f;
+    cublasHandle_t handle;
+    cudaEvent_t start, stop;
+
+    checkCudaErrors(cublasCreate(&handle));
+
+    // Perform warmup operation with cublas
+    checkCudaErrors(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));
+    cublasGemmAlgo_t algo = cublasGemmAlgo_t::CUBLAS_GEMM_ALGO4_TENSOR_OP;
+    checkCudaErrors(cublasGemmEx(handle, trans_B, trans_A, m, n, k, &alpha, d_B, CUDA_R_16F,
+        ldb, d_A, CUDA_R_16F, lda, &beta, d_C, CUDA_R_16F, ldc, CUBLAS_COMPUTE_16F, algo));
+
+    // Allocate CUDA events that we'll use for timing
+    checkCudaErrors(cudaEventCreate(&start));
+    checkCudaErrors(cudaEventCreate(&stop));
+
+    // Record the start event
+    checkCudaErrors(cudaEventRecord(start, NULL));
+
+    for (int j = 0; j < nIter; j++) {
+      // note cublas is column primary!
+      // need to transpose the order
+      checkCudaErrors(cublasGemmEx(handle, trans_B, trans_A, m, n, k, &alpha, d_B, CUDA_R_16F,
+          ldb, d_A, CUDA_R_16F, lda, &beta, d_C, CUDA_R_16F, ldc, CUBLAS_COMPUTE_16F, algo));
     }
 
-    // compute reference solution
-    printf("Computing result using host CPU...");
-    float *reference = (float *)malloc(mem_size_C);
-    matrixMulCPU(reference, h_A, h_B, matrix_size.uiHA, matrix_size.uiWA, matrix_size.uiWB);
     printf("done.\n");
 
-    // check result (CUBLAS)
-    bool resCUBLAS = sdkCompareL2fe(reference, h_CUBLAS, size_C, 1.0e-6f);
-
-    if (resCUBLAS != true) {
-        printDiff(reference, h_CUBLAS, matrix_size.uiWC, matrix_size.uiHC, 100, 1.0e-5f);
-    }
-
-    printf("Comparing CUBLAS Matrix Multiply with CPU results: %s\n", (true == resCUBLAS) ? "PASS" : "FAIL");
-
-    printf("\nNOTE: The CUDA Samples are not meant for performance measurements. "
-           "Results may vary when GPU Boost is enabled.\n");
-
-    // clean up memory
-    free(h_A);
-    free(h_B);
-    free(h_C);
-    free(reference);
-    checkCudaErrors(cudaFree(d_A));
-    checkCudaErrors(cudaFree(d_B));
-    checkCudaErrors(cudaFree(d_C));
-
-    if (resCUBLAS == true) {
-        return EXIT_SUCCESS; // return value = 1
-    }
-    else {
-        return EXIT_FAILURE; // return value = 0
-    }
+    // Record the stop event
+    checkCudaErrors(cudaEventRecord(stop, NULL));
+
+    // Wait for the stop event to complete
+    checkCudaErrors(cudaEventSynchronize(stop));
+
+    float msecTotal = 0.0f;
+    checkCudaErrors(cudaEventElapsedTime(&msecTotal, start, stop));
+
+    // Compute and print the performance
+    float msecPerMatrixMul = msecTotal / nIter;
+    double flopsPerMatrixMul = 2.0 * (double)matrix_size.uiHC *
+                               (double)matrix_size.uiWC *
+                               (double)matrix_size.uiHB;
+    double gigaFlops =
+        (flopsPerMatrixMul * 1.0e-9f) / (msecPerMatrixMul / 1000.0f);
+    printf("Performance= %.2f GFlop/s, Time= %.3f msec, Size= %.0f Ops\n",
+           gigaFlops, msecPerMatrixMul, flopsPerMatrixMul);
+
+    // copy result from device to host
+    checkCudaErrors(
+        cudaMemcpy(h_CUBLAS, d_C, mem_size_C, cudaMemcpyDeviceToHost));
+
+    // Destroy the handle
+    checkCudaErrors(cublasDestroy(handle));
+  }
+
+  // compute reference solution
+  printf("Computing result using host CPU...");
+  __half *reference = (__half *)malloc(mem_size_C);
+  matrixMulCPU(reference, h_A, h_B, matrix_size.uiHA, matrix_size.uiWA, matrix_size.uiWB, ta, tb);
+  printf("done.\n");
+
+  // check result (CUBLAS)
+  bool resCUBLAS = sdkCompareL2fe(reference, h_CUBLAS, size_C, 1.0e-2f);
+
+  if (resCUBLAS != true) {
+    printDiff(reference, h_CUBLAS, matrix_size.uiWC, matrix_size.uiHC, 100,
+              1.0e-2f);
+  }
+
+  printf("Comparing CUBLAS Matrix Multiply with CPU results: %s\n",
+         (true == resCUBLAS) ? "PASS" : "FAIL");
+
+  printf(
+      "\nNOTE: The CUDA Samples are not meant for performance measurements. "
+      "Results may vary when GPU Boost is enabled.\n");
+
+  // clean up memory
+  free(h_A);
+  free(h_B);
+  free(h_C);
+  free(reference);
+  checkCudaErrors(cudaFree(d_A));
+  checkCudaErrors(cudaFree(d_B));
+  checkCudaErrors(cudaFree(d_C));
+
+  if (resCUBLAS == true) {
+    return EXIT_SUCCESS;  // return value = 1
+  } else {
+    return EXIT_FAILURE;  // return value = 0
+  }
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 // Program main
 ////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv)
-{
-    printf("[Matrix Multiply CUBLAS] - Starting...\n");
+int main(int argc, char **argv) {
+  printf("[Matrix Multiply CUBLAS] - Starting...\n");
 
-    int         devID = 0, sizeMult = 5;
-    sMatrixSize matrix_size;
+  int devID = 0, m = 4096, n = 4096, k = 4096;
+  sMatrixSize matrix_size;
 
-    initializeCUDA(argc, argv, devID, sizeMult, matrix_size);
+  initializeCUDA(argc, argv, devID, m, n, k, matrix_size);
 
-    int matrix_result = matrixMultiply(argc, argv, devID, matrix_size);
+  int matrix_result = matrixMultiply(argc, argv, devID, matrix_size, true, false);
 
-    return matrix_result;
+  return matrix_result;
 }
-- 
2.43.0

